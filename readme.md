# Adversarial Attacks in Pytorch

This repo contains my implementation for adversarial attacks coding pratice in the Trustworthy Machine Learning course in Peking University directed by Yisen Wang, spring 2021. I implement 3 classical adversarial attack methods in Pytorch:

- [FGSM](https://arxiv.org/abs/1412.6572)
- [PGD](https://arxiv.org/pdf/1706.06083.pdf)
- [MI-FGSM](https://arxiv.org/pdf/1710.06081.pdf)


Then I pratice both whitebox and blackbox attacks on neural networks trained for CIFAR10 and MNIST classification. The target model for CIFAR10 is PreActResNet18 and the model for MNIST is a small CNN.  For more details, please see the [report](./AI_intro_adv_attack_report.pdf).





